{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 : Markov Chains\n",
    "\n",
    "### a) States and Transition Probabilities (Transition Matrix)\n",
    "   \n",
    "   $$\n",
    "   P = \n",
    "   \\begin{bmatrix}\n",
    "    & TT & ST & SS \\\\\n",
    "   TT & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n",
    "   ST & \\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} \\\\\n",
    "   SS & 0 & \\frac{1}{2} & \\frac{1}{2}\n",
    "   \\end{bmatrix}\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Probabilities corresponding to different generations\n",
    "\n",
    "**Generation 1**\n",
    "\n",
    "Probability that one would be tall in height = $\\frac{1}{4}$\n",
    "\n",
    "Probability that one would be medium in height = $\\frac{1}{2}$\n",
    "\n",
    "Probability that one would be short in height = $\\frac{1}{4}$\n",
    "\n",
    "**Generation 2**\n",
    "\n",
    "Probability that one would be tall in height = $\\frac{1}{4}$\n",
    "\n",
    "Probability that one would be medium in height = $\\frac{1}{2}$\n",
    "\n",
    "Probability that one would be short in height = $\\frac{1}{4}$\n",
    "\n",
    "**Generation 3**\n",
    "\n",
    "Probability that one would be tall in height = $\\frac{1}{4}$\n",
    "\n",
    "Probability that one would be medium in height = $\\frac{1}{2}$\n",
    "\n",
    "Probability that one would be short in height = $\\frac{1}{4}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Probabilities corresponding to different generations\n",
    "\n",
    "For any generation, the probabilities would be same, i.e,\n",
    "\n",
    "Probability that one would be tall in height = $\\frac{1}{4}$\n",
    "\n",
    "Probability that one would be medium in height = $\\frac{1}{2}$\n",
    "\n",
    "Probability that one would be short in height = $\\frac{1}{4}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 : Markov Reward Process\n",
    "\n",
    "### a) States and Transition Probabilities (Transition Matrix)\n",
    "   \n",
    "   $$\n",
    "   P = \n",
    "   \\begin{bmatrix}\n",
    "    & S & 1 & 3 & 5 & 6 & 7 & 8 & W \\\\\n",
    "   S & 0 & \\frac{1}{4} & \\frac{1}{4} & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
    "   1 & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
    "   3 & 0 & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{2} & 0 & 0 \\\\\n",
    "   5 & 0 & 0 & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
    "   6 & 0 & 0 & \\frac{1}{4} & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} \\\\\n",
    "   7 & 0 & 0 & \\frac{1}{4} & 0 & 0 & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} \\\\\n",
    "   8 & 0 & 0 & \\frac{1}{4} & 0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{4} \\\\\n",
    "   W & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Absorbing State\n",
    "\n",
    "$W$ is the Absorbing State.\n",
    "\n",
    "#### c) Expected Number of die throws\n",
    "\n",
    "Reward Function, \n",
    "$\n",
    "R = \n",
    "{\n",
    "\\begin{bmatrix}\n",
    "-9 & -8 & -6 & -4 & -3 & -2 & -1 & 2 \\\\\n",
    "\\end{bmatrix}^{\\;T}\n",
    "}\n",
    "$\n",
    "\n",
    "Discount factor,\n",
    "$\n",
    "\\gamma = 0.8\n",
    "$\n",
    "\n",
    "Bellman Equation,\n",
    "$\n",
    "V = (I - \\gamma P)^{-1}R\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-14.11333333 -12.16388889  -9.29166667  -7.41666667  -3.68055556\n",
      "  -2.68055556  -1.43055556  10.        ]\n"
     ]
    }
   ],
   "source": [
    "R = np.array([-9,-8,-6,-4,-3,-2,-1,2]).reshape((8,1))\n",
    "gamma = 0.8\n",
    "I = np.identity(8)\n",
    "P = np.array([[0,0.25,0.25,0,0,0.25,0.25,0],\n",
    "              [0,0,0.25,0.25,0,0.25,0.25,0],\n",
    "              [0,0,0,0.25,0.25,0.5,0,0],\n",
    "              [0,0,0.25,0,0.25,0.25,0.25,0],\n",
    "              [0,0,0.25,0,0,0.25,0.25,0.25],\n",
    "              [0,0,0.25,0,0,0.25,0.25,0.25],\n",
    "              [0,0,0.25,0,0,0,0.5,0.25],\n",
    "              [0,0,0,0,0,0,0,1]])\n",
    "inv = np.linalg.inv(I - gamma*P)\n",
    "V = np.squeeze(np.matmul(inv,R).T)\n",
    "print(V.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Number of Die throws = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 : Markov Decision Process\n",
    "\n",
    "### a) Graphical Representation of the MDP\n",
    "\n",
    "![MDP](MDP.jpg)\n",
    "\n",
    "**HT** = Hill Top\n",
    "**RD** = Rolling Down\n",
    "**BH** = Bottom Hill\n",
    "**DP** = Drive Probability\n",
    "**NDP** = No Drive Probability\n",
    "\n",
    "### b) Deteministic and Stochastic Policy\n",
    "\n",
    "**Deterministic Policy** = LRV can always drive with a probability 1. <br>\n",
    "**Stochastic Policy** = LRV can drive with a probability 0.5 and not drive with probability 0.5.\n",
    "\n",
    "### c) Transition Matrix\n",
    "\n",
    "**Deterministic Policy**\n",
    "    $$\n",
    "    P = \n",
    "    \\begin{bmatrix}\n",
    "    & Hill Top & Rolling Down & Bottom Hill \\\\\n",
    "    Hill Top & 0.8 & 0.2 & 0 \\\\\n",
    "    Rolling Down & 0.3 & 0.4 & 0.3 \\\\\n",
    "    Bottom Hill & 0.7 & 0.3 & 0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    \n",
    "**Stochastic Policy**\n",
    "    $$\n",
    "    P = \n",
    "    \\begin{bmatrix}\n",
    "    & Hill Top & Rolling Down & Bottom Hill \\\\\n",
    "    Hill Top & 0.7 & 0.3 & 0 \\\\\n",
    "    Rolling Down & 0.15 & 0.25 & 0.6 \\\\\n",
    "    Bottom Hill & 0.35 & 0.15 & 0.5\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    \n",
    "    \n",
    "### d) History Dependent Policy\n",
    "The LRV can perform an action based on its current energy level. If the energy level is low, then it's better not to drive and amass as much energy as possible and then decide to take an action. For example, if it feels that it's energy levels are decreasing it can go up the hill and amass energy as it absorbs the maximum amount of energy at the top of the hill and then come down later for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 : Policy Evaluation and Partial Ordering of Policies\n",
    "\n",
    "### a) Policies\n",
    "\n",
    "#### For Policy $\\pi_1$ :\n",
    "$\n",
    "V^{\\pi_1}(s = A) = 0.9[-10 + \\gamma V^{\\pi_1}(s = B)] + 0.1[-10 + \\gamma V^{\\pi_1}(s = C)] \\\\\n",
    "V^{\\pi_1}(s = B) = 0.9[-10 + 100\\gamma] + 0.1[-10 + \\gamma V^{\\pi_1}(s = A)] \\\\\n",
    "V^{\\pi_1}(s = C) = 0.1[-10 + 100\\gamma] + 0.9[-10 + \\gamma V^{\\pi_1}(s = A)] \\\\\n",
    "V^{\\pi_1}(s = D) = 100 \\\\\n",
    "$\n",
    "\n",
    "Solving the above equations and substituting $\\gamma$ = 1, <br>\n",
    "$\n",
    "V^{\\pi_1}(s = A) = 75.00 \\\\\n",
    "V^{\\pi_1}(s = B) = 87.50 \\\\\n",
    "V^{\\pi_1}(s = C) = 62.50 \\\\\n",
    "V^{\\pi_1}(s = D) = 100 \\\\\n",
    "$\n",
    "\n",
    "\n",
    "#### For Policy $\\pi_2$ :\n",
    "$\n",
    "V^{\\pi_2}(s = A) = 0.9[-10 + \\gamma V^{\\pi_2}(s = B)] + 0.1[-10 + \\gamma V^{\\pi_2}(s = C)] \\\\\n",
    "V^{\\pi_2}(s = B) = 0.1[-10 + 100\\gamma] + 0.9[-10 + \\gamma V^{\\pi_2}(s = A)] \\\\\n",
    "V^{\\pi_2}(s = C) = 0.9[-10 + 100\\gamma] + 0.1[-10 + \\gamma V^{\\pi_2}(s = A)] \\\\\n",
    "V^{\\pi_2}(s = D) = 100 \\\\\n",
    "$\n",
    "\n",
    "Solving the above equations and substituting $\\gamma$ = 1, <br>\n",
    "$\n",
    "V^{\\pi_2}(s = A) = 75.00 \\\\\n",
    "V^{\\pi_2}(s = B) = 62.50 \\\\\n",
    "V^{\\pi_2}(s = C) = 87.50 \\\\\n",
    "V^{\\pi_2}(s = D) = 100 \\\\\n",
    "$\n",
    "\n",
    "\n",
    "#### For Policy $\\pi_3$ : \n",
    "$\n",
    "V^{\\pi_3}(s = A) = 0.42[-10 + \\gamma V^{\\pi_3}(s = B)] + 0.58[-10 + \\gamma V^{\\pi_3}(s = C)] \\\\\n",
    "V^{\\pi_3}(s = B) = 0.9[-10 + 100\\gamma] + 0.1[-10 + \\gamma V^{\\pi_3}(s = A)] \\\\\n",
    "V^{\\pi_3}(s = C) = 0.9[-10 + 100\\gamma] + 0.1[-10 + \\gamma V^{\\pi_3}(s = A)] \\\\\n",
    "V^{\\pi_3}(s = D) = 100 \\\\\n",
    "$\n",
    "\n",
    "Solving the above equations and substituting $\\gamma$ = 1, <br>\n",
    "$\n",
    "V^{\\pi_3}(s = A) = 77.78 \\\\\n",
    "V^{\\pi_3}(s = B) = 87.78 \\\\\n",
    "V^{\\pi_3}(s = C) = 87.78 \\\\\n",
    "V^{\\pi_3}(s = D) = 100 \\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 : Value Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_value_function(R,gamma,c):\n",
    "    \n",
    "    V = np.array([0,0,0,0,0,10+c],dtype = np.float32)\n",
    "    epsilon = 1e-15\n",
    "    max_iterations = 100\n",
    "    \n",
    "    for itr in range(max_iterations):\n",
    "        \n",
    "        prev_V = np.copy(V)\n",
    "        for i in range(5):\n",
    "            Q_s_a = []\n",
    "            for a in range(2):\n",
    "                if a == 0:\n",
    "                    summation = V[max(i-1,0)]\n",
    "                    Q_s_a.append(R[i]+gamma*summation)\n",
    "                elif a == 1:\n",
    "                    summation = V[i+1]\n",
    "                    Q_s_a.append(R[i]+gamma*summation)\n",
    "                    \n",
    "            V[i] = np.max(Q_s_a)\n",
    "            \n",
    "        if np.sum(np.fabs(prev_V - V)) <= epsilon:\n",
    "            print('Value iteration converged at iteration %d.'%(i+1))\n",
    "            break\n",
    "            \n",
    "    return V            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Optimal Policy and Optimal Value Function at $\\gamma$ = 1\n",
    "\n",
    "Optimal Policy, the agent can always take the action R, i.e, always go right at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration 5.\n",
      "Optimal Value Function : [10. 10. 10. 10. 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "R = np.array([0,0,0,0,0,10])\n",
    "V = calc_value_function(R,1,0)\n",
    "\n",
    "print(\"Optimal Value Function :\",np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Optimal Policy and Optimal Value Function at $\\gamma$ = {0.9,0.5,0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.9\n",
      "[ 5.9   6.56  7.29  8.1   9.   10.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.5\n",
      "[ 0.31  0.62  1.25  2.5   5.   10.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.1\n",
      "[ 0.    0.    0.01  0.1   1.   10.  ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gamma in [0.9,0.5,0.1]:\n",
    "    V = calc_value_function(R,gamma,0)\n",
    "    \n",
    "    print(\"Gamma :\",gamma)\n",
    "    print(np.around(V,decimals=2))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same policy that was used above, i.e, moving right at each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Adding a constant c to all rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.9  Constant c : -15\n",
      "[-64.38 -54.87 -44.3  -32.55 -19.5   -5.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.9  Constant c : -10\n",
      "[-40.95 -34.39 -27.1  -19.   -10.     0.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.9  Constant c : -5\n",
      "[-17.52 -13.91  -9.9   -5.45  -0.5    5.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.9  Constant c : 5\n",
      "[50. 50. 50. 50. 50. 15.]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.9  Constant c : 10\n",
      "[100. 100. 100. 100. 100.  20.]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.9  Constant c : 15\n",
      "[150. 150. 150. 150. 150.  25.]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.5  Constant c : -15\n",
      "[-29.22 -28.44 -26.88 -23.75 -17.5   -5.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.5  Constant c : -10\n",
      "[-19.38 -18.75 -17.5  -15.   -10.     0.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.5  Constant c : -5\n",
      "[-9.53 -9.06 -8.12 -6.25 -2.5   5.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.5  Constant c : 5\n",
      "[10.16 10.31 10.62 11.25 12.5  15.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.5  Constant c : 10\n",
      "[20. 20. 20. 20. 20. 20.]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.5  Constant c : 15\n",
      "[30. 30. 30. 30. 30. 25.]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.1  Constant c : -15\n",
      "[-16.67 -16.67 -16.66 -16.55 -15.5   -5.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.1  Constant c : -10\n",
      "[-11.11 -11.11 -11.1  -11.   -10.     0.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.1  Constant c : -5\n",
      "[-5.56 -5.55 -5.54 -5.45 -4.5   5.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.1  Constant c : 5\n",
      "[ 5.56  5.56  5.56  5.65  6.5  15.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.1  Constant c : 10\n",
      "[11.11 11.11 11.12 11.2  12.   20.  ]\n",
      "\n",
      "\n",
      "Value iteration converged at iteration 5.\n",
      "Gamma : 0.1  Constant c : 15\n",
      "[16.67 16.67 16.67 16.75 17.5  25.  ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gamma in [0.9,0.5,0.1]:\n",
    "    for c in [-15,-10,-5,5,10,15]:\n",
    "        R = np.array([c,c,c,c,c,10+c])\n",
    "        V = calc_value_function(R,gamma,c)\n",
    "\n",
    "        print(\"Gamma :\",gamma, \" Constant c :\",c)\n",
    "        print(np.around(V,decimals=2))\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above results, as long as c is negative or zero it doesn't effect much and the optimal policy remains the same(moving right at each state). However for positive values of c, if gamma is small, then there is no problem and we can use the same optimal policy. However if gamma is large, the agent tries to explore the environment even more and in the process, the agent would not be able to reach the terminal state and moves away from it(we can see that the values decrease as we go from state s1 to s6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Relation between $V^{\\pi}$ and $\\hat{V}^{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7 : Effect of Noise and Discounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal_or_wall(i,j):\n",
    "    return ((i == 1 and j == 1) or (i == 2 and j == 1) or (i == 2 and j == 2) or (i == 2 and j == 3) or (i == 2 and j == 4) or (i == 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(R,gamma,eeta):\n",
    "    \n",
    "    V = np.array([[0,0,0,0,0],\n",
    "                  [0,0,0,0,0],\n",
    "                  [0,0,1,0,10],\n",
    "                  [0,0,0,0,0],\n",
    "                  [-10,-10,-10,-10,-10]],dtype = np.float32)\n",
    "\n",
    "    epsilon = 1e-10\n",
    "    max_iterations = 100\n",
    "\n",
    "    for itr in range(max_iterations):\n",
    "\n",
    "        prev_V = np.copy(V)\n",
    "\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                if not is_terminal_or_wall(i,j):\n",
    "                    Q_s_a = []\n",
    "                    for a in range(4):\n",
    "                        if a == 0:\n",
    "                            summation = (1-eeta)*V[i][max(j-1,0)]+(eeta/3)*V[max(i-1,0)][j]+(eeta/3)*V[min(i+1,4)][j]+(eeta/3)*V[i][min(j+1,4)]                        \n",
    "                            Q_s_a.append((1-eeta)*R[i][j]+gamma*summation)\n",
    "                        elif a == 1:\n",
    "                            summation = (eeta/3)*V[i][max(j-1,0)]+(1-eeta)*V[max(i-1,0)][j]+(eeta/3)*V[min(i+1,4)][j]+(eeta/3)*V[i][min(j+1,4)]                        \n",
    "                            Q_s_a.append((1-eeta)*R[i][j]+gamma*summation)\n",
    "                        elif a == 2:\n",
    "                            summation = (eeta/3)*V[i][max(j-1,0)]+(eeta/3)*V[max(i-1,0)][j]+(1-eeta)*V[min(i+1,4)][j]+(eeta/3)*V[i][min(j+1,4)]                        \n",
    "                            Q_s_a.append((1-eeta)*R[i][j]+gamma*summation)\n",
    "                        elif a == 3:\n",
    "                            summation = (eeta/3)*V[i][max(j-1,0)]+(eeta/3)*V[max(i-1,0)][j]+(eeta/3)*V[min(i+1,4)][j]+(1-eeta)*V[i][min(j+1,4)]                        \n",
    "                            Q_s_a.append((1-eeta)*R[i][j]+gamma*summation)\n",
    "\n",
    "                    V[i][j] = np.max(Q_s_a)\n",
    "    \n",
    "        if np.sum(np.fabs(prev_V - V)) <= epsilon:\n",
    "            print('Value iteration converged at iteration %d.'%(i+1))\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration 5.\n",
      "[[  0.     0.     0.01   0.01   0.1 ]\n",
      " [  0.     0.     0.1    0.1    0.99]\n",
      " [  0.     0.     1.     0.    10.  ]\n",
      " [ -0.     0.01   0.1    0.09   0.99]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "R = np.array([[0,0,0,0,0],\n",
    "              [0,0,0,0,0],\n",
    "              [0,0,1,0,10],\n",
    "              [0,0,0,0,0],\n",
    "              [-10,-10,-10,-10,-10]])\n",
    "\n",
    "V = value_iteration(R,0.1,0.01)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.74   2.32   3.24   4.28   5.6 ]\n",
      " [  1.24   0.     3.62   5.23   7.42]\n",
      " [  0.86   0.     1.     0.    10.  ]\n",
      " [  0.14   0.9    2.13   3.88   6.54]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.8,0.185)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.63   0.99   1.64   2.55   3.94]\n",
      " [  0.38   0.     2.19   3.7    6.21]\n",
      " [  0.21   0.     1.     0.    10.  ]\n",
      " [ -0.31   0.22   1.17   2.75   5.55]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.69,0.185)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.1    0.21   0.46   0.98   2.09]\n",
      " [  0.04   0.     0.91   2.01   4.54]\n",
      " [  0.01   0.     1.     0.    10.  ]\n",
      " [ -0.22   0.01   0.52   1.65   4.26]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.5,0.13)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.     0.     0.     0.     0.04]\n",
      " [ -0.     0.     0.06   0.04   0.63]\n",
      " [ -0.03   0.     1.     0.    10.  ]\n",
      " [ -0.52  -0.51  -0.45  -0.48   0.12]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.2,0.7)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.     0.     0.01   0.01   0.1 ]\n",
      " [  0.     0.     0.1    0.1    1.  ]\n",
      " [  0.     0.     1.     0.    10.  ]\n",
      " [  0.     0.01   0.1    0.1    1.  ]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.1,0)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.31   5.9    6.56   7.29   8.1 ]\n",
      " [  4.78   0.     7.29   8.1    9.  ]\n",
      " [  5.31   0.     1.     0.    10.  ]\n",
      " [  5.9    6.56   7.29   8.1    9.  ]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.9,0)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.     0.     0.     0.     0.03]\n",
      " [ -0.     0.     0.05   0.03   0.51]\n",
      " [ -0.     0.     1.     0.    10.  ]\n",
      " [ -0.17  -0.17  -0.12  -0.15   0.34]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.1,0.5)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.27   1.75   2.89   4.07   5.35]\n",
      " [  0.67   0.     2.43   4.11   6.96]\n",
      " [ -0.02   0.     1.     0.    10.  ]\n",
      " [ -2.14  -2.03  -1.37  -0.13   3.51]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.9,0.5)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration 5.\n",
      "[[  0.01   0.03   0.11   0.31   0.92]\n",
      " [  0.     0.     0.32   0.86   2.99]\n",
      " [ -0.02   0.     1.     0.    10.  ]\n",
      " [ -0.44  -0.42  -0.12   0.3    2.51]\n",
      " [-10.   -10.   -10.   -10.   -10.  ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(R,0.4,0.3)\n",
    "print(np.around(V,decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above observations, <br>\n",
    "**a)** If the noise factor is close to zero and the value of discount factor is also close to zero, then the agent prefers the close exit and risks the cliff. <br>\n",
    "**b)** If the noise factor is close to zero and the value of discount factor is close to one, then the agent prefers the distant exit and risks the cliff. <br>\n",
    "**c)** If the noise factor is close to one and the value of discount factor is close to zero, then the agent prefers the close exit and also avoids the cliff. <br>\n",
    "**d)** If the noise factor is close to one and the value of discount factor is also close to one, then the agent prefers the distant exit and also avoids the cliff. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8 : Convergence Rate of Value Iteration\n",
    "\n",
    "$\n",
    "V_{k+1}(s) = max_{a} [ R_{ss'}^{a} + \\gamma  {\\sum}_{s' \\in S} P_{ss'}^{a} V_{k}(s') ]\n",
    "$\n",
    "\n",
    "$\n",
    "V_{*}(s) = max_{a} [ R_{ss'}^{a} + \\gamma {\\sum}_{s' \\in S} P_{ss'}^{a} V_{*}(s') ]\n",
    "$\n",
    "\n",
    "$\n",
    "\\implies | \\; V_{k+1}(s) - V_{*}(s) \\; | \\;  \\leq | \\;  \\; \\; [ R_{ss'}^{a} + \\gamma  {\\sum}_{s' \\in S} P_{ss'}^{a} V_{k}(s') ] - [ R_{ss'}^{a} + \\gamma  {\\sum}_{s' \\in S} P_{ss'}^{a} V_{*}(s') ] \\; \\; |\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\implies | \\; V_{k+1}(s) - V_{*}(s) \\; | \\; \\leq | \\;  \\; \\;  \\gamma {\\sum}_{s' \\in S} P_{ss'}^{a} V_{k}(s')  -  \\gamma  {\\sum}_{s' \\in S} P_{ss'}^{a} V_{*}(s') \\; \\; |\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\implies | \\; V_{k+1}(s) - V_{*}(s) \\; | \\; \\leq \\gamma \\;  | \\;  \\; \\;  {\\sum}_{s' \\in S} [ P_{ss'}^{a} V_{k}(s') -P_{ss'}^{a} V_{*}(s')] \\; \\; |\n",
    "$\n",
    "\n",
    "But $ {\\sum}_{s' \\in S} P_{ss'}^{a} = I $\n",
    "\n",
    "$\n",
    "\\implies | \\; V_{k+1}(s) - V_{*}(s) \\; | \\; \\leq \\gamma \\;  | \\;  \\; \\;  {\\sum}_{s' \\in S} P_{ss'}^{a}[V_{k}(s') -V_{*}(s')] \\; \\; |\n",
    "$\n",
    "\n",
    "$\n",
    "\\implies | \\; V_{k+1}(s) - V_{*}(s) \\; | \\; \\leq \\gamma \\;  | \\;  \\; \\; [V_{k}(s') -V_{*}(s')] \\; {\\sum}_{s' \\in S}P_{ss'}^{a}\\; \\; |\n",
    "$\n",
    "\n",
    "$\n",
    "\\implies | \\; V_{k+1}(s) - V_{*}(s) \\; | \\; \\leq \\gamma \\;  | \\;  \\; \\; [V_{k}(s') -V_{*}(s')] \\; I\\; \\; |\n",
    "$\n",
    "\n",
    "$\n",
    "\\implies | \\; V_{k+1}(s) - V_{*}(s) \\; | \\; \\leq \\gamma \\;  | \\;  \\; \\; [V_{k}(s') -V_{*}(s')] \\;  \\; |\n",
    "$\n",
    "\n",
    "$\n",
    "\\implies | \\; V_{k+1}(s) - V_{*}(s) \\; | \\; \\leq \\gamma \\;  | \\;  \\; \\; [V_{k}(s') -V_{*}(s')] \\;  \\; | \\leq \\gamma \\; \\gamma \\;  | \\;  \\; \\; [V_{k-1}(s') -V_{*}(s')] \\;  \\; |\n",
    "$\n",
    "\n",
    "$\n",
    "Similarly, | \\; V_{k+1}(s) - V_{*}(s) \\; |_\\infty \\; \\leq \\gamma^{k} \\;  | \\;  \\; \\;V_{1}(s') -V_{*}(s')\\;  \\; |\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
